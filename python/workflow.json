{
    "workflow": {
        "name": "mnist_workflow",
        "tasks": [
            {
                "task": "load_data",
                "outputs": [
                    "dataset"
                ],
                "inputs": []
            },
            {
                "task": "preprocess_data",
                "outputs": [
                    "train_loader",
                    "test_loader"
                ],
                "inputs": [
                    [
                        "dataset",
                        false
                    ]
                ]
            },
            {
                "task": "train_model",
                "outputs": [
                    "model"
                ],
                "inputs": [
                    [
                        "train_loader",
                        false
                    ]
                ]
            },
            {
                "task": "evaluate_model",
                "outputs": [
                    "accuracy"
                ],
                "inputs": [
                    [
                        "model",
                        false
                    ],
                    [
                        "test_loader",
                        false
                    ]
                ]
            }
        ]
    },
    "tasks": {
        "load_data": {
            "name": "load_data",
            "code": "def load_data() -> datasets.MNIST:\n    from torchvision import datasets, transforms\n    # Load the MNIST dataset\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    return dataset",
            "inputs": [],
            "outputs": [
                "dataset"
            ],
            "container_image": "intel/deep-learning:2024.2-py3.10",
            "code_to_execute": "# Code to execute\nfrom minio import Minio\nimport io\nimport pickle\n# Initialize MinIO client\nminio_client = Minio('minio-service:9000', access_key='minio', secret_key='minio123', secure=False)\n# Create bucket if it does not exist\nexecution_id = '8faafaae-d9c9-4347-af2b-717e82be49b7'\nbuckets = [bucket.name for bucket in minio_client.list_buckets()]\nif execution_id not in buckets: minio_client.make_bucket(execution_id)\n# Serialization functions\ndef serialize_data(data):\n    import io\n    import pickle\n    data_bytes = pickle.dumps(data)\n    data_stream = io.BytesIO(data_bytes)\n    length = len(data_bytes)\n    content_type = 'application/octet-stream'\n    return data_stream, length, content_type\n\ndef deserialize_data(data_bytes):\n    import pickle\n    data = pickle.loads(data_bytes)\n    return data\n# Retrieve arguments\n\n# Call the function\nresult = load_data()\n# Store the result(s)\ndataset = result\n# Serialize and store the result\ndata_stream, length, content_type = serialize_data(dataset)\nminio_client.put_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'dataset', data_stream, length, content_type=content_type)\nprint(dataset)"
        },
        "preprocess_data": {
            "name": "preprocess_data",
            "code": "def preprocess_data(dataset: datasets.MNIST) -> Tuple[DataLoader, DataLoader]:\n    from torch.utils.data import DataLoader, random_split\n    # Split the dataset into train and test sets\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n\n    return train_loader, test_loader",
            "inputs": [
                "dataset"
            ],
            "outputs": [
                "train_loader",
                "test_loader"
            ],
            "container_image": "intel/deep-learning:2024.2-py3.10",
            "code_to_execute": "# Code to execute\nfrom minio import Minio\nimport io\nimport pickle\n# Initialize MinIO client\nminio_client = Minio('minio-service:9000', access_key='minio', secret_key='minio123', secure=False)\n# Create bucket if it does not exist\nexecution_id = '8faafaae-d9c9-4347-af2b-717e82be49b7'\nbuckets = [bucket.name for bucket in minio_client.list_buckets()]\nif execution_id not in buckets: minio_client.make_bucket(execution_id)\n# Serialization functions\ndef serialize_data(data):\n    import io\n    import pickle\n    data_bytes = pickle.dumps(data)\n    data_stream = io.BytesIO(data_bytes)\n    length = len(data_bytes)\n    content_type = 'application/octet-stream'\n    return data_stream, length, content_type\n\ndef deserialize_data(data_bytes):\n    import pickle\n    data = pickle.loads(data_bytes)\n    return data\n# Retrieve arguments\n# Retrieve argument: dataset\nresponse = minio_client.get_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'dataset')\ndata_bytes = response.read()\ndataset = deserialize_data(data_bytes)\nprint(f\"Loaded argument 'dataset'\")\n# Call the function\nresult = preprocess_data(dataset)\n# Store the result(s)\ntrain_loader, test_loader = result\n# Serialize and store 'train_loader'\ndata_stream, length, content_type = serialize_data(train_loader)\nminio_client.put_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'train_loader', data_stream, length, content_type=content_type)\nprint(train_loader)\n# Serialize and store 'test_loader'\ndata_stream, length, content_type = serialize_data(test_loader)\nminio_client.put_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'test_loader', data_stream, length, content_type=content_type)\nprint(test_loader)"
        },
        "train_model": {
            "name": "train_model",
            "code": "def train_model(train_loader: DataLoader) -> Net:\n    model = Net()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    for epoch in range(2):  # 5 epochs for demonstration\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}: [{batch_idx * len(data)}/{len(train_loader.dataset)}]'\n                      f'\\tLoss: {loss.item():.6f}')\n\n    return model",
            "inputs": [
                "train_loader"
            ],
            "outputs": [
                "model"
            ],
            "container_image": "intel/deep-learning:2024.2-py3.10",
            "code_to_execute": "# Code to execute\nfrom minio import Minio\nimport io\nimport pickle\n# Initialize MinIO client\nminio_client = Minio('minio-service:9000', access_key='minio', secret_key='minio123', secure=False)\n# Create bucket if it does not exist\nexecution_id = '8faafaae-d9c9-4347-af2b-717e82be49b7'\nbuckets = [bucket.name for bucket in minio_client.list_buckets()]\nif execution_id not in buckets: minio_client.make_bucket(execution_id)\n# Serialization functions\ndef serialize_data(data):\n    import io\n    import pickle\n    data_bytes = pickle.dumps(data)\n    data_stream = io.BytesIO(data_bytes)\n    length = len(data_bytes)\n    content_type = 'application/octet-stream'\n    return data_stream, length, content_type\n\ndef deserialize_data(data_bytes):\n    import pickle\n    data = pickle.loads(data_bytes)\n    return data\n# Retrieve arguments\n# Retrieve argument: train_loader\nresponse = minio_client.get_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'train_loader')\ndata_bytes = response.read()\ntrain_loader = deserialize_data(data_bytes)\nprint(f\"Loaded argument 'train_loader'\")\n# Call the function\nresult = train_model(train_loader)\n# Store the result(s)\nmodel = result\n# Serialize and store the result\ndata_stream, length, content_type = serialize_data(model)\nminio_client.put_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'model', data_stream, length, content_type=content_type)\nprint(model)"
        },
        "evaluate_model": {
            "name": "evaluate_model",
            "code": "def evaluate_model(model: Net, test_loader: DataLoader) -> float:\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            test_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = correct / len(test_loader.dataset)\n\n    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}')\n    return accuracy",
            "inputs": [
                "model",
                "test_loader"
            ],
            "outputs": [
                "accuracy"
            ],
            "container_image": "intel/deep-learning:2024.2-py3.10",
            "code_to_execute": "# Code to execute\nfrom minio import Minio\nimport io\nimport pickle\n# Initialize MinIO client\nminio_client = Minio('minio-service:9000', access_key='minio', secret_key='minio123', secure=False)\n# Create bucket if it does not exist\nexecution_id = '8faafaae-d9c9-4347-af2b-717e82be49b7'\nbuckets = [bucket.name for bucket in minio_client.list_buckets()]\nif execution_id not in buckets: minio_client.make_bucket(execution_id)\n# Serialization functions\ndef serialize_data(data):\n    import io\n    import pickle\n    data_bytes = pickle.dumps(data)\n    data_stream = io.BytesIO(data_bytes)\n    length = len(data_bytes)\n    content_type = 'application/octet-stream'\n    return data_stream, length, content_type\n\ndef deserialize_data(data_bytes):\n    import pickle\n    data = pickle.loads(data_bytes)\n    return data\n# Retrieve arguments\n# Retrieve argument: model\nresponse = minio_client.get_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'model')\ndata_bytes = response.read()\nmodel = deserialize_data(data_bytes)\nprint(f\"Loaded argument 'model'\")\n# Retrieve argument: test_loader\nresponse = minio_client.get_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'test_loader')\ndata_bytes = response.read()\ntest_loader = deserialize_data(data_bytes)\nprint(f\"Loaded argument 'test_loader'\")\n# Call the function\nresult = evaluate_model(model, test_loader)\n# Store the result(s)\naccuracy = result\n# Serialize and store the result\ndata_stream, length, content_type = serialize_data(accuracy)\nminio_client.put_object('8faafaae-d9c9-4347-af2b-717e82be49b7', 'accuracy', data_stream, length, content_type=content_type)\nprint(accuracy)"
        }
    },
    "imports": [
        "import torch",
        "import torch.nn as nn",
        "import torch.optim as optim",
        "from torch.utils.data import DataLoader",
        "from torchvision import datasets",
        "from typing import Dict, Tuple, List",
        "from torchvision import datasets, transforms",
        "from torch.utils.data import DataLoader, random_split"
    ],
    "top_level_definitions": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)",
        "def task(container_image: str = None):\n    def decorator(func):\n        func.container_image = container_image\n        return func\n    return decorator",
        "def decorator(func):\n        func.container_image = container_image\n        return func",
        "def workflow(func):\n    return func"
    ],
    "execution_id": "8faafaae-d9c9-4347-af2b-717e82be49b7"
}